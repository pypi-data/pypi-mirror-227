from dataclasses import dataclass
import sys
from os import path
import time
import collections, queue
import deepspeech
import pyaudio
import webrtcvad
import wave
import numpy as np
from scipy import signal
from typing import List
import multiprocessing
from multiprocessing.connection import PipeConnection


# if getattr(sys, "frozen", False):
#     CLIENT_PY_PATH = sys._MEIPASS
# else:
#     CLIENT_PY_PATH = path.join(path.dirname(path.abspath(__file__)), "../")
#     sys.path.insert(0, path.join(CLIENT_PY_PATH, "utilities"))
#     sys.path.insert(0, path.join(CLIENT_PY_PATH, "hal"))
#     sys.path.insert(0, path.join(CLIENT_PY_PATH, "middleware"))

# from Config_Manager import Config_Manager, CONFIG_FILES_DIR

from ..models import Voice, HotWord, Transcripts_Message, Candidate_Transcript_Message, Phrase

total_time = 0

def audio_log(_msg, elapsed_time = 0):
    global total_time

    if elapsed_time == 0:
        print(_msg)
        total_time = 0
    else:
        delta = millis() - elapsed_time
        total_time = total_time + delta
        print(_msg, round(delta, 2), "total", round(total_time, 2))

    return millis()

def millis():
    return time.time()

class Audio:
    """Streams raw audio from microphone. Data is received in a separate thread, and stored in a buffer, to be read from."""

    FORMAT = pyaudio.paInt16
    # Network/VAD rate-space
    SAMPLE_RATE = 16000
    CHANNELS = 1
    BLOCKS_PER_SECOND = 50

    def __init__(self, device=None, input_rate=SAMPLE_RATE, file=None):
        def proxy_callback(in_data, frame_count, time_info, status):
            # #pylint: disable=unused-argument
            # if self.chunk is not None:
            #     in_data = self.wf.readframes(self.chunk)
            # if self.populate_buffer:
            self.buffer_queue.put(in_data)
            return (None, pyaudio.paContinue)

        self.populate_buffer = False
        self.buffer_queue = queue.Queue()
        self.device = device
        self.input_rate = input_rate
        self.sample_rate = self.SAMPLE_RATE
        self.block_size = int(self.SAMPLE_RATE / float(self.BLOCKS_PER_SECOND))
        self.block_size_input = int(self.input_rate / float(self.BLOCKS_PER_SECOND))
        self.pa = pyaudio.PyAudio()

        self.stream = self.pa.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.input_rate,
            input=True,
            frames_per_buffer=self.block_size_input,
            stream_callback=proxy_callback,
            input_device_index=self.device
        )
        self.start_stream()

    def resample(self, data, input_rate):
        """
        Microphone may not support our native processing sampling rate, so
        resample from input_rate to SAMPLE_RATE here for webrtcvad and
        deepspeech

        Args:
            data (binary): Input audio stream
            input_rate (int): Input audio rate to resample from
        """
        data16 = np.fromstring(string=data, dtype=self.FORMAT)
        resample_size = int(len(data16) / self.input_rate * self.SAMPLE_RATE)
        resample = signal.resample(data16, resample_size)
        resample16 = np.array(resample, dtype=self.FORMAT)
        return resample16.tostring()

    def read_resampled(self):
        """Return a block of audio data resampled to 16000hz, blocking if necessary."""
        return self.resample(data=self.buffer_queue.get(),
                             input_rate=self.input_rate)

    def read(self):
        """Return a block of audio data, blocking if necessary."""
        return self.buffer_queue.get()

    def start_stream(self):
        self.stream.start_stream()

    def stop_stream(self):
        self.stream.stop_stream()

    def play_audio(self, filedir, filename): #PLAY WAV AUDIO GENERATED BY https://voicemaker.in/

        # self.stop_stream()

        if not self.stream.is_stopped:
            print("Cannot play", filename, "because audio stream is not stopped")
            return

        try:
            #define stream chunk   
            chunk = 1024  
            #open a wav format music
            f = wave.open(path.join(filedir, filename),"rb")  
            #instantiate PyAudio  
            p = pyaudio.PyAudio()  
            #open stream  
            stream_output = p.open(format = p.get_format_from_width(f.getsampwidth()),  
                            channels = f.getnchannels(),  
                            rate = f.getframerate(),  
                            output = True)  
            #read data  
            data = f.readframes(chunk)  

            #play stream  
            while data:  
                stream_output.write(data)  
                data = f.readframes(chunk)  

            #stop stream  
            stream_output.stop_stream()  
            stream_output.close()  

            #close PyAudio  
            p.terminate()
        except Exception as e:
            print(e)
            print("Audio file", filename, "not found.")

        # self.start_stream()

    def destroy(self):
        self.stream.stop_stream()
        self.stream.close()
        self.pa.terminate()

    frame_duration_ms = property(lambda self: 1000 * self.block_size // self.sample_rate)

class VADAudio(Audio):
    """Filter & segment audio with voice activity detection."""

    def __init__(self, aggressiveness=3, device=None, input_rate=None, file=None):
        super().__init__(device=device, input_rate=input_rate, file=file) # type: ignore
        self.vad = webrtcvad.Vad(aggressiveness)

    def frame_generator(self):
        """Generator that yields all audio frames from microphone."""
        if self.input_rate == self.SAMPLE_RATE:
            while True:
                yield self.read()
        else:
            while True:
                yield self.read_resampled()

    def vad_collector(self, padding_ms=300, ratio=0.75, frames=None):
        """Generator that yields series of consecutive audio frames comprising each utterence, separated by yielding a single None.
            Determines voice activity by ratio of frames in padding_ms. Uses a buffer to include padding_ms prior to being triggered.
            Example: (frame, ..., frame, None, frame, ..., frame, None, ...)
                      |---utterence---|        |---utterence---|
        """
        if frames is None: frames = self.frame_generator()
        num_padding_frames = padding_ms // self.frame_duration_ms
        ring_buffer = collections.deque(maxlen=num_padding_frames)
        triggered = False

        for frame in frames:
            if len(frame) < 640:
                return

            is_speech = self.vad.is_speech(frame, self.sample_rate)

            if not triggered:
                ring_buffer.append((frame, is_speech))
                num_voiced = len([f for f, speech in ring_buffer if speech])
                if num_voiced > ratio * num_padding_frames:
                    triggered = True
                    for f, s in ring_buffer:
                        yield f
                    ring_buffer.clear()

            else:
                yield frame
                ring_buffer.append((frame, is_speech))
                num_unvoiced = len([f for f, speech in ring_buffer if not speech])
                if num_unvoiced > ratio * num_padding_frames:
                    triggered = False
                    yield None
                    ring_buffer.clear()


class Tactigon_Speech(multiprocessing.Process):
    """Tactigon_Speech process. Get a stream of audio input (from a microphone or a TSkin) and put all the results into a pipe

    Args:
        pipe (Connection): the pipe where to put the transcript containing all the candidates found.
    """

    RUNNING = 1
    STOPPED = 2

    def __init__(self, config: Voice, pipe: PipeConnection, debug: bool = True):
        super(Tactigon_Speech, self).__init__(
            target=self._loop,
            args=(config,pipe,debug,),
        )
    
    def _loop(self, config: Voice, pipe: PipeConnection, debug):


        model_file = config.model_full_path
        scorer_file = config.scorer_full_path
        vad_aggressiveness = config.vad_aggressiveness
        vad_padding_ms = config.vad_padding_ms
        vad_ratio = config.vad_ratio
        rate = Audio.SAMPLE_RATE
        min_sample_len = config.min_sample_len
        confidence_threshold = 1
        # min_transcript_len = config.get("min_transcript_len")
        # max_transcript_len = config.get("max_transcript_len")
        num_transcript = config.num_transcript
        beam_width = config.beam_width
        default_phrases_cfg = config.default_phrases
        audio_file_location = config.audio_file_location
        # hot_words = config.get("hot_words")
        phrases = []
        # Loading models and scorer

        model_file = model_file
        model = deepspeech.Model(model_file)
        
        if scorer_file:
            scorer_file = scorer_file
            model.enableExternalScorer(scorer_file)

        if beam_width:
            model.setBeamWidth(beam_width)

        default_phrases = [Phrase([HotWord(hw["word"], hw["boost"]) for hw in phrase["hot_words"]]) for phrase in default_phrases_cfg]

        # Start audio with VAD
        vad_audio = VADAudio(aggressiveness=vad_aggressiveness, input_rate=rate)
        frames = vad_audio.vad_collector(padding_ms=vad_padding_ms, ratio=vad_ratio)

        # Stream from microphone to DeepSpeech using VAD
        audio_stream = bytearray()

        for frame in frames:
            if frame is not None:
                audio_stream.extend(frame)
            else:

                result = None
                timeout = False
                audio_stream = np.frombuffer(audio_stream, np.int16) # type: ignore

                if len(audio_stream) < min_sample_len:
                    audio_stream = bytearray()
                    continue

                vad_audio.stop_stream()
                
                if pipe.poll():
                    # check if got new hot words
                    pipe_phrases = []
                    while pipe.poll():
                        # I can have mmultiple phrases, i want to look always for the last one
                        pipe_phrases = pipe.recv()

                    try:
                        phrases = [p for p in pipe_phrases if not p.is_default]
                        _default_phrases = [p for p in pipe_phrases if p.is_default]
                        if len(_default_phrases) > 0:
                            default_phrases = _default_phrases
                    except:
                        # i've something in the pipe that's not a list of Phrases
                        pass

                if len(phrases) == 0:
                    phrases = default_phrases

                # load custon hot words and run inference
                for phrase in phrases:
                    model.clearHotWords()

                    if pipe.poll():
                        pipe_msg = pipe.recv()
                        if isinstance(pipe_msg, TimeoutError):
                            timeout = True
                            break


                    hot_words_list = []
                    for hw in phrase.hot_words:
                        hot_words_list.append(hw.word)
                        model.addHotWord(hw.word, hw.boost)

                    sst_metadata = model.sttWithMetadata(audio_stream, num_transcript)
                    transcript_message = Transcripts_Message.from_metadata(sst_metadata, confidence_threshold)

                    
                    if debug:
                        print(transcript_message)

                    if transcript_message.check_against_hotwords(hot_words_list):
                        result = phrase
                        break

                if timeout:
                    pipe.send(TimeoutError)
                else:
                    pipe.send(result)
                    
                    if len(audio_stream) > 0:
                        if result == None:
                            print("error")
                            if phrases[0].error_feedback:
                                vad_audio.play_audio(audio_file_location, phrases[0].error_feedback)
                        else:
                            print("ok")
                            # send phrase feedback
                            if result.audio_feedback:
                                vad_audio.play_audio(audio_file_location, result.audio_feedback)

                vad_audio.start_stream()
                audio_stream = bytearray()

    def stop(self):
        super().terminate()



# def phrase_routine(pipe, phrase: Phrase):
#     pipe.send(phrase.hot_words)

#     print(F"Say: {phrase.phrase}")

#     while not pipe.poll():
#         time.sleep(0.1)
    
#     res = pipe.recv()

#     if res:
#         phrase.n_correct += 1
#     else:
#         phrase.n_incorrect += 1

#     print(phrase.score)
#     print("Ok!")
#     return phrase

if __name__ == "__main__":
    import json
    from os import path

    with open(path.join(path.dirname(__file__), "config_files", "voice.json")) as voice_file:
        voice: Voice = Voice.FromJSON("", json.load(voice_file))

    pipe_1, pipe_2 = multiprocessing.Pipe()

    t_speech = Tactigon_Speech(voice, pipe_2)

    t_speech.start()

    # # frase1 = Phrase("wake up tgear", [HotWord("wake"), HotWord("up"), HotWord("gear")])
    # # frase2_1 = Phrase("I’m teaching you a lesson", [HotWord("teaching"), HotWord("you"), HotWord("lesson")])
    # # frase2_2 = Phrase("Repeat a lesson please", [HotWord("repeat"), HotWord("lesson"), HotWord("please")])
    # # frase2_3 = Phrase("Exit program", [HotWord("exit", 20), HotWord("program", 20)])
    # # frase3 = Phrase("Say hello to our friends", [HotWord("hello"), HotWord("our"), HotWord("friend")])
    # # frase4 = Phrase("Wave to them", [HotWord("wave"), HotWord("them")])
    # # frase5 = Phrase("Bow down to them", [HotWord("bow"), HotWord("down"), HotWord("them")])
    # # frase6 = Phrase("Turn to another friend", [HotWord("turn"), HotWord("another"), HotWord("friend")])
    # # frase7 = Phrase("Move forward", [HotWord("move"), HotWord("forward")])

    wake_up = Phrase([HotWord("wake"), HotWord("gear")], is_default=True, audio_feedback="how_can_i_help_you.wav")

    # LET'S TRAIN KUNG FU FIGHT
    # LET'S TRAIN TO FIGHT
    # kung_fu = Phrase([HotWord("train"), HotWord("fight")])

    # # LET'S DANCE TOGETHER
    # dance_rock = Phrase([HotWord("dance"), HotWord("together")])

    # # CATCH THE BALL FROM FLOOR
    # catch_ball = Phrase([HotWord("ball"), HotWord("floor")])

    # ph = [wake_up, kung_fu, dance_rock, catch_ball]

    ph = [wake_up]


    teacher = Phrase([HotWord("teach"), HotWord("lesson")], audio_feedback="choose_a_lesson.wav", error_feedback="i_didnt_understand.wav")
    student = Phrase([HotWord("repeat"), HotWord("learn")], audio_feedback="choose_a_lesson.wav")
    exit_p = Phrase([HotWord("exit"), HotWord("program")], audio_feedback="exiting_program.wav")

    while True:
        pipe_1.send(ph)
        while not pipe_1.poll():
            time.sleep(0.1)

        res = pipe_1.recv()
        print(res)

        # if res == wake_up:
        #     pipe_1.send([teacher, student, exit_p])
        #     _t = 0
        #     while _t < 10:
        #         if not pipe_1.poll():
        #             _t += 0.1
        #             time.sleep(0.1)
        #             continue

        #         res = pipe_1.recv()
        #         print("choose", res)

        #         if res == teacher:
        #             print("teacher")
        #         elif res == student:
        #             print("student")
        #         elif res == exit_p:
        #             print("exit")
        #         else:
        #             print("dunno")

        # pipe_1.send([Phrase([HotWord("teaching"), HotWord("you"), HotWord("lesson")]), Phrase([HotWord("repeat"), HotWord("lesson"), HotWord("please")])])
        
        # while not pipe_1.poll():
        #     time.sleep(0.1)

        # res = pipe_1.recv()
        # print(res)

        # pipe_1.send([Phrase([HotWord("teaching"), HotWord("you"), HotWord("lesson")]), Phrase([HotWord("repeat"), HotWord("lesson"), HotWord("please")])])
        
        # while not pipe_1.poll():
        #     time.sleep(0.1)

        # res = pipe_1.recv()
        # print(res)

    # t_speech.stop()

    

