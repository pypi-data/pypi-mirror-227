{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81954e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh import palettes\n",
    "import bokeh.palettes\n",
    "\n",
    "from bokeh.io import output_file\n",
    "from bokeh.models import (CDSView, ColumnDataSource, CustomJS, GroupFilter, FactorRange, \n",
    "                          HoverTool, Legend, LegendItem)\n",
    "from bokeh.models.formatters import FuncTickFormatter\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from itertools import cycle\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Imports from this package.\n",
    "from ghostbokeh import GhostBokeh\n",
    "from slideselect import SlideSelect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea4fb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to(data, baseline, to=100):\n",
    "    \"\"\"\n",
    "    Scale data so values at `baseline` map to `to`\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    # Index (2001 = 100)\n",
    "    df = pd.DataFrame(dict(year=[2000, 2001, 2002], jobs=[40, 50, 20]))\n",
    "    baseline = df.jobs[df.year == 2001].values[0]\n",
    "    df[\"jobs_index\"] = index_to(df.jobs, baseline)\n",
    "    df\n",
    "    #    year  jobs  jobs_index\n",
    "    # 0  2000    40        80.0\n",
    "    # 1  2001    50       100.0\n",
    "    # 2  2002    20        40.0\n",
    "    \"\"\"\n",
    "    \n",
    "    return data / baseline * to\n",
    "\n",
    "def growth_pct_from(data, baseline):\n",
    "    \"\"\"\n",
    "    Percentage growth from baseline data\n",
    "    \n",
    "    ## Year on year growth\n",
    "    df = pd.DataFrame(dict(year=[2000, 2001, 2002], jobs=[40, 50, 20]))\n",
    "    baseline = df.jobs[df.year == 2001].values[0]\n",
    "    df[\"jobs_yoy\"] = growth_pct_from(df, baseline)\n",
    "    \n",
    "    ## Cumulative growth for two columns\n",
    "    df = pd.DataFrame(dict(\n",
    "        year=[2000, 2001, 2002], \n",
    "        jobs=[40, 50, 20], \n",
    "        gva=[200, 250, 275]))\n",
    "    baseline = df.loc[df.year == df.year.min(), (\"jobs\", \"gva\")].reindex(index=df.index, method=\"nearest\")\n",
    "    df[[\"jobs_growth\", \"gva_growth\"]] = growth_pct_from(df[[\"jobs\", \"gva\"]], baseline)\n",
    "    df\n",
    "    \"\"\"\n",
    "    \n",
    "    return (data / baseline - 1) * 100\n",
    "\n",
    "\n",
    "def _cumulative_growth(data, columns, date_var=\"date\"):\n",
    "    # Wrap single column name in a list, for convenience.\n",
    "    columns = [columns] if isinstance(columns, str) else columns\n",
    "    \n",
    "    # Classify each row as having the earliest date or not.\n",
    "    is_min_date = data[date_var] == data[date_var].min()\n",
    "    \n",
    "    # Calculate baseline for each column from row with earliest date.\n",
    "    baseline = data.loc[is_min_date, columns] \\\n",
    "        .reindex(index=data.index, method=\"nearest\")  # Broadcast baseline to match shape of data.\n",
    "    return growth_pct_from(data[columns],\n",
    "                           baseline)\n",
    "\n",
    "def growth_vars(data, columns=[], date_var=None, by=None, \n",
    "                periods=1, baseline=None):\n",
    "    \"\"\"\n",
    "    # Period-on-period\n",
    "    growth_vars(df, columns=[\"gva\"], date_var=\"date\", periods=1)\n",
    "    \n",
    "    # Cumulative growth\n",
    "    growth_vars(df, columns=[\"gva\"], date_var=\"date\", baseline=\"first\")\n",
    "\n",
    "    # Growth relative to single date.\n",
    "    growth_vars(df, columns=[\"gva\"], date_var=\"date\", baseline=\"2019 Q4\")\n",
    "    \n",
    "    # Revisions from comparable dataframe.\n",
    "    growth_vars(df, columns=[\"gva\"], baseline=df_baseline)\n",
    "\n",
    "\n",
    "    # Growth relative to single date, with a split factor.\n",
    "    growth_vars(df, columns=[\"gva\"], date_var=\"date\", by=\"industry\", baseline=\"2019 Q4\")\n",
    "    \n",
    "    # Same as:\n",
    "    baseline = data.loc[df[\"date\"]==min(df[\"date\"]), :].groupby(\"industry\")[\"gva\"].first()\n",
    "    growth_vars(df, columns=[\"gva\"], date_var=\"date\", baseline=baseline)\n",
    "\n",
    "    \n",
    "    # Relative to calculated baseline for each level of `by`\n",
    "    baseline = data.loc[data[\"year\"]==\"2019\", :].groupby(\"industry\")[\"gva\"].mean()\n",
    "    growth_vars(df, columns=[\"gva\"], by=\"industry\", baseline=baseline)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure data columns include the ones we need.\n",
    "    if date_var is not None:\n",
    "        assert date_var in data.columns\n",
    "    if by is not None:\n",
    "        assert by in data.columns\n",
    "    assert all(col in data.columns for col in columns), \\\n",
    "        f\"Some of {columns} are missing from data columns {data.columns}\"\n",
    "\n",
    "    \n",
    "    # Make placeholder copy of data ready to inject results into `columns`.\n",
    "    result = data.copy()\n",
    "    result[columns] = np.nan\n",
    "    \n",
    "    # Expand baseline shortcuts (\"first\" or a value to match data[date_var]).\n",
    "    if baseline == \"first\":\n",
    "        # Put baseline at earliest date value to get cumulative growth.\n",
    "        if by is not None:\n",
    "            baseline = data.loc[data[date_var]==min(data[date_var]), :].groupby(by)[columns].first()\n",
    "        else:\n",
    "            baseline = data.loc[data[date_var]==min(data[date_var]), :]\n",
    "    elif baseline is not None and not isinstance(baseline, pd.DataFrame):\n",
    "        # Find column values from date_var == baseline.\n",
    "        df_baseline_columns = columns + [by] if by is not None else columns\n",
    "        df_baseline_raw = data.set_index(date_var).loc[baseline, df_baseline_columns]\n",
    "        if by is not None:\n",
    "            # Take mean for each of the columns at each level of `by`.\n",
    "            baseline = df_baseline_raw.groupby(by).mean()\n",
    "        else:\n",
    "            # Take mean for each of the columns.\n",
    "            baseline = df_baseline_raw.mean()\n",
    "    elif isinstance(baseline, pd.DataFrame):\n",
    "        # Expect baseline dataframe to have a value for each column, with optional `by` splits.\n",
    "        pass\n",
    "    else:\n",
    "        assert baseline is None\n",
    "    \n",
    "    if isinstance(baseline, pd.DataFrame):\n",
    "        # Get relative change of data compared to baseline dataframe.\n",
    "\n",
    "        assert all(col in baseline.columns for col in columns), \\\n",
    "            f\"Some of {columns} are missing from baseline columns {baseline.columns}\"\n",
    "\n",
    "        if by is not None and date_var not in baseline.columns:\n",
    "            # Use `by` to look up baseline rows to find baseline values for columns.\n",
    "            #  `by` dataframe should have index of `by` levels.\n",
    "            baseline_values = baseline.loc[data[by], columns].values\n",
    "        else:\n",
    "            # Align baseline to data, and compare baseline dataframe to columns.\n",
    "            join_columns = [date_var, by] if by is not None else [date_var]\n",
    "            join_keys = list(pd.MultiIndex.from_frame(data[join_columns]))\n",
    "            baseline_values = baseline.set_index(join_columns).loc[join_keys, columns].values\n",
    "\n",
    "        result[columns] = growth_pct_from(data[columns],\n",
    "                                          baseline=baseline_values)\n",
    "    else:\n",
    "        # Do period-on-period growth with each column.\n",
    "        if by is not None:\n",
    "            sorted_data = data.sort_values(date_var).groupby(by)[columns]\n",
    "        else:\n",
    "            sorted_data = data[columns].sort_values(date_var)\n",
    "        result[columns] = 100 * sorted_data.pct_change(periods=periods)\n",
    "    return result\n",
    "\n",
    "\n",
    "def zz_growth_vars(data, columns=[], reverse=[], by=None, \n",
    "                suffix=\"\", reverse_suffix=\"\",\n",
    "                method=\"total\", date_var=\"date\"):\n",
    "    \n",
    "    # Make result dataframe with columns for date_var and by.\n",
    "    result = data[[date_var] + ([] if by is None else [by])].copy()\n",
    "    \n",
    "    if method == \"total\":\n",
    "        if by is None:\n",
    "            # Each column is a single time series with no splits.\n",
    "            growth = _cumulative_growth(data, columns=columns, date_var=date_var)\n",
    "        else:\n",
    "            # Calculate growth for each split group.\n",
    "            growth = data.groupby(by) \\\n",
    "                .apply(_cumulative_growth, columns=columns, date_var=date_var)\n",
    "    else:\n",
    "        raise NotImplemented(f\"Method {method} not implemented\")\n",
    "\n",
    "    # Put growth into result dataframe.\n",
    "    result[columns] = growth\n",
    "\n",
    "    # Wrap single reverse column name in list, for convenience.\n",
    "    reverse = [reverse] if isinstance(reverse, str) else reverse\n",
    "    if len(reverse):\n",
    "        # Reverse sign of growth for specified variables.\n",
    "        result[reverse] *= -1\n",
    "\n",
    "    if len(suffix):\n",
    "        # Append suffix to column names.\n",
    "        result.rename(\n",
    "            inplace=True,\n",
    "            columns={\n",
    "                name: name + suffix for name in columns\n",
    "            })\n",
    "    \n",
    "    if len(reverse_suffix):\n",
    "        # Append reverse_suffix to names of columns with sign-reversed data.\n",
    "        result.rename(\n",
    "            inplace=True,\n",
    "            columns={\n",
    "                name + suffix: name + suffix + reverse_suffix for name in reverse\n",
    "            })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7f355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_fill(keys, values):\n",
    "    \"\"\"\n",
    "    Map keys to values, recycling values as necessary\n",
    "    \"\"\"\n",
    "    \n",
    "    return {key: value for key, value in zip(keys, cycle(values))}\n",
    "\n",
    "def variables_cmap(variables, palette):\n",
    "    \"\"\"\n",
    "    Map variables to colors\n",
    "    \n",
    "    If there are more variables than colors in the palette,\n",
    "    colors are recycled.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    variables: str or list[str]\n",
    "        Variable name or list of names.\n",
    "    palette: str or array\n",
    "        Named palette from Bokeh.palettes, or array of colors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict mapping variable names to colors.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(variables, str):\n",
    "        # Wrap simple string in a list, for convenience.\n",
    "        variables = [variables]\n",
    "    n_data_series = len(variables)\n",
    "    \n",
    "    if isinstance(palette, str):\n",
    "        # Access named palette from bokeh.palettes.\n",
    "        palette = getattr(palettes, palette)\n",
    "    \n",
    "    if isinstance(palette, dict):\n",
    "        # Extract color palette from palette dict, by number of colors needed.\n",
    "        last_palette = [palette.values()][-1]\n",
    "        palette = palette.get(n_data_series, last_palette)\n",
    "\n",
    "    # Map variables to palette colors, recycling colors as needed.\n",
    "    color_map = dict_fill(keys=variables, values=palette)\n",
    "    return color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efffc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iv_dv_figure(\n",
    "    iv_data = None,\n",
    "    iv_axis = \"x\",\n",
    "    legend = \"default\",\n",
    "    legend_place = \"center\",\n",
    "    suppress_factors = False,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Return default figure options, updated with optional keywords\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    iv_data: array or series\n",
    "        Independent variable against which data will be plotted.  If the\n",
    "        data provided satisfy pandas `is_datetime()`, the relevant axis type \n",
    "        will be \"datetime\".  Otherwise the default axis type will be used,\n",
    "        with categorical values and a factor range determined by the unique\n",
    "        data values.  For regular time periods like annual, quarterly, or\n",
    "        monthly economic data, a categorical axis is often easier to work\n",
    "        with and format than a datetime axis.\n",
    "    iv_axis: str, default \"x\"\n",
    "        Whether the independent variable is to be plotted against the \"x\"\n",
    "        (horizontal) or \"y\" (vertical) axis.\n",
    "    keywords : mapping, optional\n",
    "        Override default options.\n",
    "    Returns\n",
    "    -------\n",
    "    Bokeh `Figure`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default figure options.\n",
    "    fopts = dict(\n",
    "        background_fill_color = \"#fafafa\",\n",
    "        tools = \"reset,save,pan,box_zoom,wheel_zoom\",\n",
    "    )\n",
    "    if iv_data is not None:\n",
    "        # Specify option to configure independent axis.\n",
    "        if is_datetime(iv_data):\n",
    "            # Not recommended, but try to accommodate it.\n",
    "            key = iv_axis + \"_axis_type\"\n",
    "            fopts[key] = \"datetime\"\n",
    "        else:\n",
    "            # Use categorical axis (x or y).\n",
    "            axis_range = pd.Series(iv_data).unique()  # Do not sort.\n",
    "            key = iv_axis + \"_range\"\n",
    "            fopts[key] = FactorRange(factors=axis_range,\n",
    "                                     factor_padding = 0.2,\n",
    "                                     group_padding = 0.2,\n",
    "                                     subgroup_padding = 0.2)\n",
    "\n",
    "    # Fold in explicit options to override others.\n",
    "    fopts.update(kwargs)\n",
    "    fig = figure(**fopts)\n",
    "    \n",
    "    if suppress_factors:\n",
    "        # Suppress most lowest level categorical tick labels.\n",
    "        # If tick labels are tuples, higher levels will be displayed\n",
    "        # as normal.\n",
    "        tf_margins_only = FuncTickFormatter(\n",
    "            code=\"\"\"\n",
    "            if ((index == 0) | (index == ticks.length - 1)) {\n",
    "                return tick;\n",
    "            } else {\n",
    "                return '';\n",
    "            }\n",
    "            \"\"\"\n",
    "        )\n",
    "        axis = fig.xaxis if iv_axis == \"x\" else fig.yaxis\n",
    "        axis[0].formatter = tf_margins_only\n",
    "\n",
    "    if legend is not None:\n",
    "        if legend == \"default\":\n",
    "            legend = Legend(\n",
    "                location = \"top_left\",\n",
    "                background_fill_alpha = 0.0)  # Transparent.\n",
    "        fig.add_layout(legend, place=legend_place)\n",
    "    \n",
    "    fig.toolbar.logo = None\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f73329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class FactorView(GhostBokeh, CDSView):\n",
    "#    pass\n",
    "\n",
    "def factor_filters(by, source=None, name_template=\"filter\"):\n",
    "    \"\"\"\n",
    "    Return list of GroupFilter objects for specified variables\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    by: str, sequence, or dict\n",
    "        Categorical variables to filter by, or a mapping\n",
    "        from variable names to initial values to use in the\n",
    "        corresponding filters.  The `dict` form must be\n",
    "        used if `source` is not given.\n",
    "    source: ColumnDataSource or DataFrame, optional\n",
    "        Data to filter.  Ignored if `by` is a `dict`.  Required\n",
    "        if `by` is not a `dict`.\n",
    "    name_template: str, optional\n",
    "        Combined with each `by` variable to assign a name to\n",
    "        the corresponding filter.  The default is \"filter\",\n",
    "        assigning names of the form \"filter_X\", \"filter_Y\",\n",
    "        and so forth, where \"X\" and \"Y\" are among the `by`\n",
    "        variables.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of filters that can be used with CDSView tofilter\n",
    "    a `ColumnDataSource` on values of the `by` variables.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    data = pd.DataFrame.from_records(\n",
    "        [(\"A\", 2001, 10),\n",
    "         (\"A\", 2002, 15),\n",
    "         (\"B\", 2001, 20),\n",
    "         (\"B\", 2002, 18)],\n",
    "         columns=[\"industry\", \"year\", \"sales\"]\n",
    "    )\n",
    "    cds = ColumnDataSource(data)\n",
    "    filters = factor_filters(\"industry\", source=cds)\n",
    "    view = CDSView(source=cds, filters=filters)\n",
    "    \n",
    "    # Explicit initial filter value.\n",
    "    filters = factor_filters({\"industry\": \"B\"})\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(by, str):\n",
    "        # Wrap in list, for convenience.\n",
    "        by = [by]\n",
    "    \n",
    "    if not isinstance(by, dict):\n",
    "        # Map `by` variables to initial values to use in filter.\n",
    "        data = (source.data if isinstance(source, ColumnDataSource) else data)\n",
    "        by = {key: next(iter(data[key])) for key in by}\n",
    "\n",
    "    filters = [\n",
    "        GroupFilter(\n",
    "            column_name=var,\n",
    "            group=initial,\n",
    "            name=\"_\".join([name_template, var])\n",
    "        ) \\\n",
    "        for var, initial in by.items() \n",
    "    ]\n",
    "    return filters\n",
    "\n",
    "\n",
    "def factor_view(source, by, **kwargs):\n",
    "    \"\"\"\n",
    "    Return a CDSView to filter source on specified variables\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    source : ColumnDataSource\n",
    "        Data to filter.\n",
    "    by : str or sequence of str\n",
    "        Categorical variables to filter by.\n",
    "    kwargs : mapping, optional\n",
    "        Keyword arguments passed into `factor_filter()`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A CSDView that filters `source` on values of the `by` variables.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    data = pd.DataFrame.from_records(\n",
    "        [(\"A\", 2001, 10),\n",
    "         (\"A\", 2002, 15),\n",
    "         (\"B\", 2001, 20),\n",
    "         (\"B\", 2002, 18)],\n",
    "         columns=[\"industry\", \"year\", \"sales\"]\n",
    "    )\n",
    "    cds = ColumnDataSource(data)\n",
    "    view = factor_view(cds, \"category\")\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(source, ColumnDataSource), f\"source must be ColumnDataSource, not {type(source)}\"\n",
    "    \n",
    "    view = CDSView(\n",
    "        source=source,\n",
    "        filters=factor_filters(by, source=source, **kwargs)\n",
    "    )\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8bc6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_widgets_to_groupfilters(widgets, view=None, source=None, filters=None):\n",
    "    \"\"\"\n",
    "    Link values of widgets to corresponding GroupFilter objects\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    widgets : Bokeh widget or list of widgets\n",
    "        The `value` property of each widget will be linked to the\n",
    "        `group` property of the corresponding `GroupFilter`.  If there are\n",
    "        more widgets than filters, the excess widgets are ignored.\n",
    "    view : CDSView, optional\n",
    "        Provides `source` and `filters` if they are not specified directly.\n",
    "    source : ColumnDataSource\n",
    "        Data source, which will be configured to emit a change signal when\n",
    "        a filter's `group` property changes, to re-render the relevant figure.\n",
    "    filters : GroupFilter or sequence of GroupFilter\n",
    "        The `group` property of each filter will be updated whenever the\n",
    "        `value` of the corresponding widget changes, and `source` will emit\n",
    "        a change signal whenever the `group` property of a filter changes.  If\n",
    "        there are more filters than widgets, excess filters are ignored.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    from bokeh.models import ColumnDataSource\n",
    "    from base import factor_view\n",
    "    source = ColumnDataSource({\"industry\": [\"A\", \"B\"],\n",
    "                               \"growth\": [10, 12]})\n",
    "    view_by_factor = factor_view(source, \"industry\")\n",
    "    widget = SlideSelect(options=[\"A\", \"B\"],\n",
    "                         name=\"industry_filter\")  # Show this in a layout.\n",
    "    link_widgets_to_groupfilters(widget, \n",
    "                                 view=view_by_factor)\n",
    "    \n",
    "    # Source and filter can be specified directly.\n",
    "    link_widgets_to_groupfilters(widget, \n",
    "                                 source=source,\n",
    "                                 filters=view_by_factor.filters)\n",
    "    \"\"\"\n",
    "\n",
    "    if all(x is None for x in (view, source, filters)):\n",
    "        raise ValueError(\"Must either specify source and filters, or view\")\n",
    "    \n",
    "    if source is None:\n",
    "        source = view.source\n",
    "    \n",
    "    if filters is None:\n",
    "        # Find filters in view, assumed to correspond to widgets.\n",
    "        filters = view.filters\n",
    "    \n",
    "    # Wrap singleton widget or filter, for convenience.\n",
    "    if not isinstance(widgets, (list, tuple)):\n",
    "        widgets = [widgets]\n",
    "    if not isinstance(filters, (list, tuple)):\n",
    "        filters = [filters]\n",
    "        \n",
    "    for widget, filt in zip(widgets, filters):\n",
    "        # Link widget to the GroupFilter.\n",
    "        assert isinstance(filt, GroupFilter)\n",
    "        widget.js_link(\"value\", other=filt, other_attr=\"group\")\n",
    "\n",
    "        # Signal change in data when filter `group` attribute changes, \n",
    "        # so chart refreshes.\n",
    "        filt.js_on_change(\n",
    "            \"group\",\n",
    "            CustomJS(args=dict(source=source),\n",
    "                     code=\"\"\"\n",
    "                         source.change.emit()\n",
    "                     \"\"\"))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105f62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_legend_items(fig, renderers=None, items=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Add legend items to figure\n",
    "    \n",
    "    Extends the legend items of a Bokeh figure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fig : Bokeh Figure\n",
    "        Figure to add legend items to.\n",
    "    renderers : mapping\n",
    "        Mapping of labels to renderers, to create `LegendItem`\n",
    "        objects.  Each value should be a renderer or list of renderers.\n",
    "        Either `renderers` or `items` must be specified.\n",
    "        The `renderers` parameter is ignored if `items` are given.  \n",
    "    items : list of LegendItem\n",
    "        Will be added to the figure's legend items.\n",
    "        Either `renderers` or `items` must be specified.\n",
    "    kwargs : mapping, optional\n",
    "        Keyword arguments passed to `fig.Legend` if\n",
    "        `fig` does not already have a legend.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If neither renderers nor items is given.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    from bokeh.io import show\n",
    "    from bokeh.models import Legend\n",
    "    from bokeh.plotting import figure\n",
    "    fig = figure()\n",
    "    fig.add_layout(Legend(location=\"top_left\",\n",
    "                          background_fill_alpha=0.0))\n",
    "    plot = fig.circle(x=1, y=3)\n",
    "    extend_legend_items(fig, {\"x\": plot})\n",
    "    show(fig)\n",
    "    \"\"\"\n",
    "    \n",
    "    if renderers is None and items is None:\n",
    "        raise ValueError(\"either renderers or items required\")\n",
    "        \n",
    "    if items is None and renderers is not None:        \n",
    "        # Make a legend item for each renderer.\n",
    "        items = [\n",
    "            # Include legend item for each factor level.\n",
    "            LegendItem(\n",
    "                label=var, \n",
    "                renderers=renderer if isinstance(renderer, list) else [renderer], \n",
    "            ) \\\n",
    "            for var, renderer in renderers.items()\n",
    "        ]\n",
    "    \n",
    "    fig.legend.items.extend(items)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7910a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hover_tool(fig, renderers, *tooltips, simplify=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Add a hover tool to a Bokeh figure, for given renderers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fig : Bokeh Figure\n",
    "        Figure to add hover tool to.\n",
    "    renderers : list\n",
    "        Renderers that should trigger the hover tool.\n",
    "    tooltips : list or dict\n",
    "        Positional arguments should be (label, value) tuples for a\n",
    "        tabular hover tool.  Alternatively, `tooltips` can be\n",
    "        given as a keyword argument assigned to a mapping of labels\n",
    "        to values.\n",
    "    simplify : bool, default True\n",
    "        Suppress the label of a single (label, value) tooltip, so the\n",
    "        hover tool shows just the formatted value.  The `simplify`\n",
    "        flag has no effect if multiple tooltips are given.\n",
    "    kwargs : mapping, optional\n",
    "        Additional keyword arguments are passed to `Hovertool()`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(tooltips, dict):\n",
    "        # Convert mapping to list of (label, value) tuples.\n",
    "        tooltips = tooltips.items()\n",
    "    \n",
    "    tooltips = list(tooltips)  # Coerce to list from *args tuple, or .items().\n",
    "    if len(tooltips) == 1 and simplify:\n",
    "        # Just use the tooltip string without a tabular label.\n",
    "        _, tooltips = tooltips[0]\n",
    "    \n",
    "    hover_tool = HoverTool(\n",
    "        tooltips=tooltips,\n",
    "        renderers=renderers,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    fig.add_tools(hover_tool)\n",
    "    return hover_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63f1b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# App helper functions that should be moved elsewhere.\n",
    "\n",
    "def set_output_file(outfile, title):\n",
    "    \"\"\"\n",
    "    Set Bokeh output file for standalone application\n",
    "    \n",
    "    Filename suffix is coerced to 'html'\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    set_output_file(args.save or args.datafile, \"OPH by industry\")\n",
    "    \"\"\"\n",
    "    \n",
    "    outfile = Path(outfile).with_suffix(\".html\").as_posix()\n",
    "    output_file(outfile, title=title, mode='inline')\n",
    "\n",
    "def unpack_data_varnames(args, arg_names, defaults=None):\n",
    "    \"\"\"\n",
    "    Look up command line arguments or defaults\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assemble CL arguments, which default to None.\n",
    "    mapping = {arg: getattr(args, arg) for arg in arg_names}\n",
    "    if (defaults is not None\n",
    "        and all(arg is None for arg in mapping.values())):\n",
    "        # Use default names.\n",
    "        mapping = dict(zip(arg_names, defaults))\n",
    "    return mapping\n",
    "\n",
    "def date_tuples(dates, length_threshold = np.inf):\n",
    "    \"\"\"\n",
    "    Coerce monthly, quarterly, or annual dates to tuples\n",
    "    \n",
    "    Tuples (if converted to a list) are suitable for `bokeh` categorical axis\n",
    "    \n",
    "    Parameters\n",
    "    dates : Series of str\n",
    "        Dates which may be annual ('2021'), quarterly ('2021 Q3'), or\n",
    "        monthly ('March 2021' or anything recognised by Pandas \n",
    "        `.to_period()`).\n",
    "    length_threshold : integer, default np.inf\n",
    "        If the number of unique `dates` exceeds this threshold, only the\n",
    "        last two digits of years are used.  The default is to always\n",
    "        use four-digit years.  If 0 is given, only the last two digits\n",
    "        of years will be used, regardless of how many different `dates`\n",
    "        there are.\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_date = dates[0]\n",
    "    n_dates = len(dates.unique())\n",
    "    \n",
    "    if re.fullmatch(\"\\d{4}\", sample_date):\n",
    "        # Annual like '2019', use as is.\n",
    "        if n_dates > length_threshold:\n",
    "            # Keep only last two digits of year.\n",
    "            tdate = [year[-2:] for year in dates]\n",
    "        else:\n",
    "            tdate = list(dates)\n",
    "        return tdate\n",
    "\n",
    "    if re.fullmatch(\"\\d{4} ?Q\\d\", sample_date.upper()):\n",
    "        # Quarterly like '2019Q3' or '2019 Q3'.\n",
    "        # Wrap in a tuple for Bokeh categorical axis.\n",
    "        tdate = dates.str.split(\" \").apply(tuple)\n",
    "    else:\n",
    "        # Maybe monthly will work.\n",
    "        # Create canonical (year, Mmm) category via datetime.\n",
    "        dt_dates = pd.to_datetime(dates).dt.to_period(\"M\")\n",
    "        tdate = list(zip(dt_dates.dt.year.astype(str), dt_dates.dt.month.apply('M{:02d}'.format)))\n",
    "        \n",
    "    if n_dates > length_threshold:\n",
    "        # Keep only last two digits of year.\n",
    "        tdate = [(year[-2:], _) for (year, _) in tdate]\n",
    "    return tdate\n",
    "\n",
    "def filter_widget(options, title=None, start_value=\"first\"):\n",
    "    \"\"\"\n",
    "    Make a widget to select among values of a sequence\n",
    "    \"\"\"\n",
    "\n",
    "    if title is None:\n",
    "        try:\n",
    "            title = options.name\n",
    "        except AttributeError:\n",
    "            title = \"option\"\n",
    "    # Get unique options into a list, respecting order of appearance.\n",
    "    options = list(pd.Series(options).unique())\n",
    "    widget = SlideSelect(options=options,\n",
    "                         title=title,  # Shown.\n",
    "                         name=title + \"_filter\")  # Internal.\n",
    "    if start_value == \"last\":\n",
    "        widget.value = widget.options[-1]  # Start at last value.\n",
    "\n",
    "    return widget\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
