# ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# ---------------------------------------------------------
import base64
import json
import logging
import os
import time
import traceback
import yaml

from azure.identity import DefaultAzureCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchField, SearchFieldDataType, SearchableField, PrioritizedFields,
    SemanticSettings, SemanticConfiguration, SemanticField,
    VectorSearch, VectorSearchAlgorithmConfiguration
)
from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount
from azureml.rag.embeddings import EmbeddingsContainer
from azureml.rag.mlindex import MLIndex
from azureml.rag.utils.connections import get_connection_credential
from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity, _logger_factory, version
from pathlib import Path
from typing import Optional


logger = get_logger('update_acs')

_azure_logger = logging.getLogger('azure.core.pipeline')
_azure_logger.setLevel(logging.WARNING)

acs_user_agent = f"azureml-rag=={version}/pipeline"


def search_client_from_config(acs_config: dict, credential):
    return SearchClient(endpoint=acs_config['endpoint'],
                        index_name=acs_config['index_name'],
                        credential=credential,
                        api_version=acs_config.get('api_version', '2023-07-01-preview'),
                        user_agent=acs_user_agent)


def create_search_index_sdk(acs_config: dict, credential, embeddings: Optional[EmbeddingsContainer] = None):
    logger.info(f"Ensuring search index {acs_config['index_name']} exists")
    index_client = SearchIndexClient(endpoint=acs_config['endpoint'], credential=credential, user_agent=acs_user_agent)
    if acs_config['index_name'] not in index_client.list_index_names():
        fields = [
            SimpleField(name="id", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),
            SearchableField(name="content", type=SearchFieldDataType.String, analyzer_name="en.microsoft"),
            SearchableField(name="category", type=SearchFieldDataType.String, filterable=True, facetable=True),
            SimpleField(name="sourcepage", type=SearchFieldDataType.String, filterable=True, facetable=True),
            SimpleField(name="sourcefile", type=SearchFieldDataType.String, filterable=True, facetable=True),
            SearchableField(name="title", type=SearchFieldDataType.String, filterable=True, facetable=True),
            SimpleField(name="content_hash", type=SearchFieldDataType.String, filterable=True, facetable=True),
            SimpleField(name="meta_json_string", type=SearchFieldDataType.String, filterable=True, facetable=True)
        ]

        vector_search = None
        if str(acs_config.get('push_embeddings', 'true')).lower() == "true" and embeddings and embeddings.kind != "none":
            field_name = f"content_vector_{embeddings.kind}"
            logger.info(f"Adding '{field_name}' to search index")
            fields.append(SearchField(
                name=field_name,
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                searchable=True,
                vector_search_dimensions=embeddings.get_embedding_dimensions(),
                vector_search_configuration=f"{field_name}_config"))

            vector_search = VectorSearch(
                algorithm_configurations=[
                    VectorSearchAlgorithmConfiguration(
                        name=f"{field_name}_config",
                        kind="hnsw",
                        hnsw_parameters={
                            "m": 4,
                            "efConstruction": 400,
                            "efSearch": 500,
                            "metric": "cosine"
                        }
                    )
                ]
            )

        semantic_config = SemanticConfiguration(
            name="azureml-default",
            prioritized_fields=PrioritizedFields(
                title_field=SemanticField(field_name="title"),
                prioritized_keywords_fields=[SemanticField(field_name="category")],
                prioritized_content_fields=[SemanticField(field_name="content")]
            )
        )

        index = SearchIndex(
            name=acs_config['index_name'],
            fields=fields,
            semantic_settings=SemanticSettings(configurations=[semantic_config]),
            vector_search=vector_search
        )

        logger.info(f"Creating {acs_config['index_name']} search index")
        result = index_client.create_or_update_index(index)
        logger.info(f"Created {result.name} search index")
    else:
        logger.info(f"Search index {acs_config['index_name']} already exists")


def create_index_from_raw_embeddings(emb: EmbeddingsContainer, acs_config={}, connection={}, output_path: Optional[str] = None) -> MLIndex:
    with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:
        logger.info("Updating ACS index")

        credential = get_connection_credential(connection)

        create_search_index_sdk(acs_config, credential, emb)

        search_client = search_client_from_config(acs_config, credential)

        batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100

        def process_upload_results(results, start_time):
            succeeded = []
            failed = []
            for r in results:
                if isinstance(r, dict):
                    if r['status'] is False:
                        failed.append(r)
                    else:
                        succeeded.append(r)
                else:
                    if r.succeeded:
                        succeeded.append(r)
                    else:
                        failed.append(r)
            duration = time.time() - start_time
            logger.info(f"Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed")
            activity_logger.info("Uploaded documents", extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})
            if len(failed) > 0:
                for r in failed:
                    if isinstance(r, dict):
                        error = r['errorMessage']
                    else:
                        error = r.error_message
                    logger.error(f"Failed document reason: {error}")
                return failed
            return []

        t1 = time.time()
        num_source_docs = 0
        batch = []
        for doc_id, emb_doc in emb._document_embeddings.items():
            logger.info(f'Adding document: {doc_id}')
            acs_doc = {
                "@search.action": "mergeOrUpload",
                "id": base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'),
                "content": emb_doc.get_data(),
                "category": emb_doc.metadata.get("category", "document"),
                "sourcepage": emb_doc.metadata.get("source", {}).get("url"),
                "sourcefile": emb_doc.metadata.get("source", {}).get("filename"),
                "title": emb_doc.metadata.get("source", {}).get("title"),
                "content_hash": emb_doc.document_hash,
                "meta_json_string": json.dumps(emb_doc.metadata),
            }

            if str(acs_config.get('push_embeddings', 'true')).lower() == "true" and emb and emb.kind != "none":
                acs_doc[f"content_vector_{emb.kind}"] = emb_doc.get_embeddings()

            batch.append(acs_doc)
            if len(batch) % batch_size == 0:
                logger.info(f"Sending {len(batch)} documents to ACS")
                start_time = time.time()
                results = search_client.upload_documents(batch)
                failed = process_upload_results(results, start_time)
                if len(failed) > 0:
                    logger.info(f"Retrying {len(failed)} documents")
                    failed_ids = [fail['key'] for fail in failed]
                    results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])
                    failed = process_upload_results(results, start_time)
                    if len(failed) > 0:
                        raise RuntimeError(f"Failed to upload {len(failed)} documents.")
                batch = []
                num_source_docs += batch_size

        if len(batch) > 0:
            logger.info(f"Sending {len(batch)} documents to ACS")
            start_time = time.time()
            results = search_client.upload_documents(batch)
            failed = process_upload_results(results, start_time)
            if len(failed) > 0:
                logger.info(f"Retrying {len(failed)} documents")
                failed_ids = [fail['key'] for fail in failed]
                results = search_client.upload_documents([doc for doc in batch if doc['id'] in failed_ids])
                failed = process_upload_results(results, start_time)
                if len(failed) > 0:
                    raise RuntimeError(f"Failed to upload {len(failed)} documents.")

            num_source_docs += len(batch)

        duration = time.time()-t1
        logger.info(f"Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds")
        activity_logger.info("Built index", extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})
        activity_logger.activity_info['num_source_docs'] = num_source_docs

        logger.info('Writing MLIndex yaml')
        mlindex_config = {
            "embeddings": emb.get_metadata()
        }
        mlindex_config["index"] = {
            "kind": "acs",
            "engine": "azure-sdk",
            "index": acs_config['index_name'],
            "api_version": acs_config.get('api_version', '2023-07-01-preview'),
            "field_mapping": {
                "content": "content",
                "url": "sourcepage",
                "filename": "sourcefile",
                "title": "title",
                "metadata": "meta_json_string",
            },
            "semantic_configuration_name": "azureml-default"
        }
        if str(acs_config.get('push_embeddings', 'true')).lower() == "true" and emb and emb.kind != "none":
            mlindex_config["index"]["field_mapping"]["embedding"] = f"content_vector_{emb.kind}"

        if not isinstance(connection, DefaultAzureCredential):
            mlindex_config["index"] = {**mlindex_config["index"], **connection}

        # Keyvault auth and Default ambient auth need the endpoint, Workspace Connection auth could get endpoint.
        mlindex_config["index"]["endpoint"] = acs_config['endpoint']

        if output_path is not None:
            output = Path(output_path)
            output.mkdir(parents=True, exist_ok=True)
            with open(output / "MLIndex", "w") as f:
                yaml.dump(mlindex_config, f)

    mlindex = MLIndex(mlindex_config=mlindex_config)
    return mlindex


def main(args, logger, activity_logger):
    try:
        try:
            acs_config = json.loads(args.acs_config)
        except Exception as e:
            logger.error(f'Failed to parse acs_config as json: {e}')
            activity_logger.error("Failed to parse acs_config as json")
            raise

        connection_args = {}
        connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_ACS')
        if connection_id is not None:
            connection_args['connection_type'] = 'workspace_connection'
            connection_args['connection'] = {'id': connection_id}
            from azureml.rag.utils.connections import get_connection_by_id_v2

            connection = get_connection_by_id_v2(connection_id)
            acs_config['endpoint'] = connection['properties']['target']
            acs_config['api_version'] = connection['properties'].get('metadata', {}).get('apiVersion', "2023-07-01-preview")
        elif 'endpoint_key_name' in acs_config:
            connection_args['connection_type'] = 'workspace_keyvault'
            from azureml.core import Run
            run = Run.get_context()
            ws = run.experiment.workspace
            connection_args['connection'] = {
                'key': acs_config['endpoint_key_name'],
                "subscription": ws.subscription_id,
                "resource_group": ws.resource_group,
                "workspace": ws.name,
            }

        # Mount embeddings container and create index from it
        raw_embeddings_uri = args.embeddings
        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')
        splits = raw_embeddings_uri.split('/')
        embeddings_dir_name = splits.pop(len(splits)-2)
        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')
        parent = '/'.join(splits)
        logger.info(f'extracted embeddings container path: {parent}')

        mnt_options = MountOptions(
            default_permission=0o555, allow_other=False, read_only=True)
        logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/embeddings_mount')
        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:
            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)
            create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output)

    except Exception as e:
        logger.error('Failed to update ACS index')
        exception_str = str(e)
        if 'Floats quota has been exceeded for this service.' in exception_str:
            logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')
            logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')
            activity_logger.activity_info['error_classification'] = 'UserError'
            activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Floats quota has been exceeded for this service."
        elif 'Cannot find nested property' in exception_str:
            logger.error(f'The vector index provided "{acs_config["index_name"]}" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.')
            activity_logger.activity_info['error_classification'] = 'UserError'
            activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Cannot find nested property"
        elif 'Failed to upload' in exception_str:
            activity_logger.activity_info['error'] = exception_str
            activity_logger.activity_info['error_classification'] = 'SystemError'
        else:
            activity_logger.activity_info['error'] = str(e.__class__.__name__)
            activity_logger.activity_info['error_classification'] = 'SystemError'
        raise e

    logger.info('Updated ACS index')


def main_wrapper(args, logger):
    with track_activity(logger, "update_acs") as activity_logger:
        try:
            main(args, logger, activity_logger)
        except Exception:
            activity_logger.error(f"update_acs failed with exception: {traceback.format_exc()}")  # activity_logger doesn't log traceback
            raise


if __name__ == '__main__':
    from argparse import ArgumentParser

    parser = ArgumentParser()
    parser.add_argument("--embeddings", type=str)
    parser.add_argument("--acs_config", type=str)
    parser.add_argument("--output", type=str)
    args = parser.parse_args()

    print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))

    enable_stdout_logging()
    enable_appinsights_logging()

    try:
        main_wrapper(args, logger)
    finally:
        if _logger_factory.appinsights:
            _logger_factory.appinsights.flush()
            time.sleep(5)
